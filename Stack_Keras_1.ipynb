{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "polish-lindsay",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import keras\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dense\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import LayerNormalization\n",
    "from keras.models import Sequential\n",
    "\n",
    "import yellowbrick\n",
    "from yellowbrick.datasets import load_concrete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "excessive-background",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy: 1.21.5\n",
      "pandas: 1.4.1\n",
      "sklearn: 1.0.2\n",
      "keras: 2.7.0\n",
      "yellowbrick: 1.4\n"
     ]
    }
   ],
   "source": [
    "print(\"numpy:\", np.__version__)\n",
    "print(\"pandas:\", pd.__version__)\n",
    "print(\"sklearn:\", sklearn.__version__)\n",
    "print(\"keras:\", keras.__version__)\n",
    "print(\"yellowbrick:\", yellowbrick.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "inner-richardson",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_concrete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "lined-flour",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cement</th>\n",
       "      <th>slag</th>\n",
       "      <th>ash</th>\n",
       "      <th>water</th>\n",
       "      <th>splast</th>\n",
       "      <th>coarse</th>\n",
       "      <th>fine</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>540.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1040.0</td>\n",
       "      <td>676.0</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>540.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1055.0</td>\n",
       "      <td>676.0</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>332.5</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>332.5</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>198.6</td>\n",
       "      <td>132.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>978.4</td>\n",
       "      <td>825.5</td>\n",
       "      <td>360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1025</th>\n",
       "      <td>276.4</td>\n",
       "      <td>116.0</td>\n",
       "      <td>90.3</td>\n",
       "      <td>179.6</td>\n",
       "      <td>8.9</td>\n",
       "      <td>870.1</td>\n",
       "      <td>768.3</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1026</th>\n",
       "      <td>322.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>115.6</td>\n",
       "      <td>196.0</td>\n",
       "      <td>10.4</td>\n",
       "      <td>817.9</td>\n",
       "      <td>813.4</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027</th>\n",
       "      <td>148.5</td>\n",
       "      <td>139.4</td>\n",
       "      <td>108.6</td>\n",
       "      <td>192.7</td>\n",
       "      <td>6.1</td>\n",
       "      <td>892.4</td>\n",
       "      <td>780.0</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1028</th>\n",
       "      <td>159.1</td>\n",
       "      <td>186.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>175.6</td>\n",
       "      <td>11.3</td>\n",
       "      <td>989.6</td>\n",
       "      <td>788.9</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1029</th>\n",
       "      <td>260.9</td>\n",
       "      <td>100.5</td>\n",
       "      <td>78.3</td>\n",
       "      <td>200.6</td>\n",
       "      <td>8.6</td>\n",
       "      <td>864.5</td>\n",
       "      <td>761.5</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1030 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      cement   slag    ash  water  splast  coarse   fine  age\n",
       "0      540.0    0.0    0.0  162.0     2.5  1040.0  676.0   28\n",
       "1      540.0    0.0    0.0  162.0     2.5  1055.0  676.0   28\n",
       "2      332.5  142.5    0.0  228.0     0.0   932.0  594.0  270\n",
       "3      332.5  142.5    0.0  228.0     0.0   932.0  594.0  365\n",
       "4      198.6  132.4    0.0  192.0     0.0   978.4  825.5  360\n",
       "...      ...    ...    ...    ...     ...     ...    ...  ...\n",
       "1025   276.4  116.0   90.3  179.6     8.9   870.1  768.3   28\n",
       "1026   322.2    0.0  115.6  196.0    10.4   817.9  813.4   28\n",
       "1027   148.5  139.4  108.6  192.7     6.1   892.4  780.0   28\n",
       "1028   159.1  186.7    0.0  175.6    11.3   989.6  788.9   28\n",
       "1029   260.9  100.5   78.3  200.6     8.6   864.5  761.5   28\n",
       "\n",
       "[1030 rows x 8 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "delayed-prospect",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       79.986111\n",
       "1       61.887366\n",
       "2       40.269535\n",
       "3       41.052780\n",
       "4       44.296075\n",
       "          ...    \n",
       "1025    44.284354\n",
       "1026    31.178794\n",
       "1027    23.696601\n",
       "1028    32.768036\n",
       "1029    32.401235\n",
       "Name: strength, Length: 1030, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ongoing-boost",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, stratify=None, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "patient-projection",
   "metadata": {},
   "outputs": [],
   "source": [
    "def K_Regre(): \n",
    "    keras.backend.clear_session()\n",
    "#   neural network architecture: start  \n",
    "    model = Sequential() \n",
    "    model.add(Dense(15, activation='relu')) \n",
    "    model.add(BatchNormalization()) \n",
    "#    model.add(LayerNormalization()) \n",
    "    model.add(Dense(1)) \n",
    "#   neural network architecture: end   \n",
    "    model.compile(loss='mean_squared_error', optimizer='adam') \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "subsequent-portsmouth",
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='auto', verbose=1, patience=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dressed-louisiana",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-15-e2c7326b8c95>:1: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead.\n",
      "  K_R = KerasRegressor(K_Regre,batch_size=64, epochs=2000, callbacks=[es],validation_data=(X_test, y_test), verbose=1)\n"
     ]
    }
   ],
   "source": [
    "K_R = KerasRegressor(K_Regre,batch_size=64, epochs=2000, callbacks=[es],validation_data=(X_test, y_test), verbose=1) \n",
    "K_R._estimator_type = 'regressor'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "jewish-magnitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "level_0 = [ \n",
    "          ('ELNE', ElasticNet(alpha=0.01, l1_ratio=0.5)), \n",
    "          ('KERR', K_R), \n",
    "          ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "sustained-discharge",
   "metadata": {},
   "outputs": [],
   "source": [
    "level_1 = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "satisfied-acrylic",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = StackingRegressor(level_0, final_estimator=level_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "missing-surgery",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "11/11 [==============================] - 1s 31ms/step - loss: 1595.4772 - val_loss: 2020.3481\n",
      "Epoch 2/2000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 1589.6898 - val_loss: 1849.2654\n",
      "Epoch 3/2000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 1583.2938 - val_loss: 1774.8831\n",
      "Epoch 4/2000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 1576.5785 - val_loss: 1730.1853\n",
      "Epoch 5/2000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 1569.4993 - val_loss: 1702.2384\n",
      "Epoch 6/2000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 1561.2898 - val_loss: 1676.2925\n",
      "Epoch 7/2000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1552.8201 - val_loss: 1647.3811\n",
      "Epoch 8/2000\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 1543.7280 - val_loss: 1622.1842\n",
      "Epoch 9/2000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1533.0184 - val_loss: 1609.1033\n",
      "Epoch 10/2000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1522.8292 - val_loss: 1580.0941\n",
      "Epoch 11/2000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 1511.5339 - val_loss: 1549.9001\n",
      "Epoch 12/2000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 1498.6154 - val_loss: 1529.1929\n",
      "Epoch 13/2000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1485.6530 - val_loss: 1486.7750\n",
      "Epoch 14/2000\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 1471.8608 - val_loss: 1448.8931\n",
      "Epoch 15/2000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 1457.5632 - val_loss: 1416.3984\n",
      "Epoch 16/2000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1442.6973 - val_loss: 1378.1793\n",
      "Epoch 17/2000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 1427.4359 - val_loss: 1331.9714\n",
      "Epoch 18/2000\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 1410.8521 - val_loss: 1300.2760\n",
      "Epoch 19/2000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 1393.4570 - val_loss: 1256.7697\n",
      "Epoch 20/2000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 1375.5343 - val_loss: 1202.7390\n",
      "Epoch 21/2000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 1358.2365 - val_loss: 1166.3783\n",
      "Epoch 22/2000\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 1339.8004 - val_loss: 1126.2996\n",
      "Epoch 23/2000\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 1321.4247 - val_loss: 1079.5988\n",
      "Epoch 24/2000\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 1304.3540 - val_loss: 1044.0004\n",
      "Epoch 25/2000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 1283.4432 - val_loss: 993.9413\n",
      "Epoch 26/2000\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 1264.8138 - val_loss: 949.9286\n",
      "Epoch 27/2000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 1243.3060 - val_loss: 928.3804\n",
      "Epoch 28/2000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 1224.1311 - val_loss: 888.4141\n",
      "Epoch 29/2000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 1202.3500 - val_loss: 867.2395\n",
      "Epoch 30/2000\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 1184.0607 - val_loss: 845.4886\n",
      "Epoch 31/2000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1164.3690 - val_loss: 788.2599\n",
      "Epoch 32/2000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 1139.8453 - val_loss: 782.7816\n",
      "Epoch 33/2000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 1118.3274 - val_loss: 777.1483\n",
      "Epoch 34/2000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 1098.5981 - val_loss: 741.1152\n",
      "Epoch 35/2000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1078.9762 - val_loss: 684.9790\n",
      "Epoch 36/2000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1055.7986 - val_loss: 689.0014\n",
      "Epoch 37/2000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 1034.1222 - val_loss: 665.3793\n",
      "Epoch 38/2000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1012.1781 - val_loss: 626.5198\n",
      "Epoch 39/2000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 992.8147 - val_loss: 589.5147\n",
      "Epoch 40/2000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 970.1044 - val_loss: 576.5415\n",
      "Epoch 41/2000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 950.4508 - val_loss: 532.1277\n",
      "Epoch 42/2000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 927.8733 - val_loss: 489.7178\n",
      "Epoch 43/2000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 911.3339 - val_loss: 463.2126\n",
      "Epoch 44/2000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 887.2568 - val_loss: 435.8665\n",
      "Epoch 45/2000\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 871.9973 - val_loss: 447.4142\n",
      "Epoch 46/2000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 843.7038 - val_loss: 407.3287\n",
      "Epoch 47/2000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 824.3342 - val_loss: 403.8871\n",
      "Epoch 48/2000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 804.0410 - val_loss: 392.4883\n",
      "Epoch 49/2000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 782.2849 - val_loss: 408.7980\n",
      "Epoch 50/2000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 764.2256 - val_loss: 385.8028\n",
      "Epoch 51/2000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 742.7215 - val_loss: 385.6387\n",
      "Epoch 52/2000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 724.3480 - val_loss: 387.1170\n",
      "Epoch 53/2000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 704.6093 - val_loss: 398.0677\n",
      "Epoch 54/2000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 689.8536 - val_loss: 372.3790\n",
      "Epoch 55/2000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 671.7462 - val_loss: 330.5516\n",
      "Epoch 56/2000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 649.5787 - val_loss: 362.3779\n",
      "Epoch 57/2000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 631.4822 - val_loss: 356.4761\n",
      "Epoch 58/2000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 610.6838 - val_loss: 358.4415\n",
      "Epoch 59/2000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 593.6077 - val_loss: 361.8334\n",
      "Epoch 60/2000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 576.8378 - val_loss: 357.3717\n",
      "Epoch 61/2000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 555.8658 - val_loss: 346.8322\n",
      "Epoch 62/2000\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 538.9374 - val_loss: 346.9996\n",
      "Epoch 63/2000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 527.6037 - val_loss: 334.2352\n",
      "Epoch 64/2000\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 506.3514 - val_loss: 339.0452\n",
      "Epoch 65/2000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 490.7031 - val_loss: 350.0353\n",
      "Epoch 66/2000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 474.9027 - val_loss: 276.1527\n",
      "Epoch 67/2000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 460.4093 - val_loss: 296.1868\n",
      "Epoch 68/2000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 443.7429 - val_loss: 322.4906\n",
      "Epoch 69/2000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 425.2211 - val_loss: 295.5653\n",
      "Epoch 70/2000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 408.8310 - val_loss: 285.0290\n",
      "Epoch 71/2000\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 399.0009 - val_loss: 265.2879\n",
      "Epoch 72/2000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 380.4317 - val_loss: 287.9748\n",
      "Epoch 73/2000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 366.7233 - val_loss: 280.9130\n",
      "Epoch 74/2000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 350.8641 - val_loss: 272.4018\n",
      "Epoch 75/2000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 335.7753 - val_loss: 252.8014\n",
      "Epoch 76/2000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 319.0979 - val_loss: 260.0959\n",
      "Epoch 77/2000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 307.6339 - val_loss: 268.0529\n",
      "Epoch 78/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 9ms/step - loss: 293.9218 - val_loss: 264.1549\n",
      "Epoch 79/2000\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 284.1435 - val_loss: 219.1869\n",
      "Epoch 80/2000\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 267.9100 - val_loss: 230.9834\n",
      "Epoch 81/2000\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 257.6327 - val_loss: 230.4518\n",
      "Epoch 82/2000\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 246.4520 - val_loss: 255.5000\n",
      "Epoch 83/2000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 231.2897 - val_loss: 222.1291\n",
      "Epoch 84/2000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 223.7023 - val_loss: 228.1414\n",
      "Epoch 85/2000\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 214.2912 - val_loss: 210.9411\n",
      "Epoch 86/2000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 204.2153 - val_loss: 228.7777\n",
      "Epoch 87/2000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 192.8080 - val_loss: 199.9225\n",
      "Epoch 88/2000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 185.4199 - val_loss: 178.7925\n",
      "Epoch 89/2000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 175.8620 - val_loss: 198.3312\n",
      "Epoch 90/2000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 169.5270 - val_loss: 173.7531\n",
      "Epoch 91/2000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 159.9447 - val_loss: 170.4447\n",
      "Epoch 92/2000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 152.5698 - val_loss: 151.9088\n",
      "Epoch 93/2000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 144.0110 - val_loss: 152.1733\n",
      "Epoch 94/2000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 141.2441 - val_loss: 127.4735\n",
      "Epoch 95/2000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 133.7108 - val_loss: 142.7038\n",
      "Epoch 96/2000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 125.2717 - val_loss: 116.0828\n",
      "Epoch 97/2000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 120.8273 - val_loss: 120.3259\n",
      "Epoch 98/2000\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 117.9734 - val_loss: 100.3237\n",
      "Epoch 99/2000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 112.0669 - val_loss: 104.5654\n",
      "Epoch 100/2000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 105.8609 - val_loss: 97.4732\n",
      "Epoch 101/2000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 106.5005 - val_loss: 101.2625\n",
      "Epoch 102/2000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 95.7172 - val_loss: 87.3360\n",
      "Epoch 103/2000\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 92.4836 - val_loss: 91.9588\n",
      "Epoch 104/2000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 91.5435 - val_loss: 86.7291\n",
      "Epoch 105/2000\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 88.7140 - val_loss: 85.4146\n",
      "Epoch 106/2000\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 83.4345 - val_loss: 65.1224\n",
      "Epoch 107/2000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 81.7291 - val_loss: 73.5773\n",
      "Epoch 108/2000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 79.4252 - val_loss: 65.8278\n",
      "Epoch 109/2000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 76.4138 - val_loss: 79.2971\n",
      "Epoch 110/2000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 73.5728 - val_loss: 58.8932\n",
      "Epoch 111/2000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 67.3828 - val_loss: 67.2976\n",
      "Epoch 112/2000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 65.1742 - val_loss: 59.0245\n",
      "Epoch 113/2000\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 66.0760 - val_loss: 55.5418\n",
      "Epoch 114/2000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 62.5971 - val_loss: 60.3697\n",
      "Epoch 115/2000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 62.9374 - val_loss: 58.0532\n",
      "Epoch 116/2000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 60.6632 - val_loss: 50.7190\n",
      "Epoch 117/2000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 57.4424 - val_loss: 59.5657\n",
      "Epoch 118/2000\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 56.1691 - val_loss: 51.1291\n",
      "Epoch 119/2000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 54.9893 - val_loss: 54.7472\n",
      "Epoch 120/2000\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 53.6943 - val_loss: 51.1596\n",
      "Epoch 121/2000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 55.9481 - val_loss: 51.1859\n",
      "Epoch 122/2000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 51.1951 - val_loss: 47.5593\n",
      "Epoch 123/2000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 52.0614 - val_loss: 49.3694\n",
      "Epoch 124/2000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 53.7889 - val_loss: 53.6589\n",
      "Epoch 125/2000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 50.6949 - val_loss: 48.0099\n",
      "Epoch 126/2000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 47.4920 - val_loss: 48.8762\n",
      "Epoch 127/2000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 48.0810 - val_loss: 47.4925\n",
      "Epoch 128/2000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 48.7167 - val_loss: 44.9322\n",
      "Epoch 129/2000\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 48.7477 - val_loss: 45.6608\n",
      "Epoch 130/2000\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 48.4246 - val_loss: 44.2606\n",
      "Epoch 131/2000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 44.8064 - val_loss: 45.1690\n",
      "Epoch 132/2000\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 47.0184 - val_loss: 45.2226\n",
      "Epoch 133/2000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 45.8649 - val_loss: 45.5499\n",
      "Epoch 134/2000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 44.8735 - val_loss: 44.3049\n",
      "Epoch 135/2000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 46.3483 - val_loss: 45.4053\n",
      "Epoch 136/2000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 43.4272 - val_loss: 44.5629\n",
      "Epoch 137/2000\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 40.7841 - val_loss: 44.7033\n",
      "Epoch 138/2000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 46.5221 - val_loss: 44.7319\n",
      "Epoch 139/2000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 45.1263 - val_loss: 43.8281\n",
      "Epoch 140/2000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 45.1326 - val_loss: 44.9275\n",
      "Epoch 141/2000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 42.5314 - val_loss: 43.9486\n",
      "Epoch 142/2000\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 44.3629 - val_loss: 43.7930\n",
      "Epoch 143/2000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 41.7136 - val_loss: 45.5838\n",
      "Epoch 144/2000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 46.7928 - val_loss: 45.6529\n",
      "Epoch 145/2000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 41.4869 - val_loss: 43.4630\n",
      "Epoch 146/2000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 44.0990 - val_loss: 45.4585\n",
      "Epoch 147/2000\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 42.8640 - val_loss: 43.7438\n",
      "Epoch 148/2000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 39.2010 - val_loss: 45.2732\n",
      "Epoch 149/2000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 44.6082 - val_loss: 44.2415\n",
      "Epoch 150/2000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 47.6712 - val_loss: 44.4462\n",
      "Epoch 151/2000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 41.2380 - val_loss: 43.2476\n",
      "Epoch 152/2000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 43.3718 - val_loss: 45.7052\n",
      "Epoch 153/2000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 39.3936 - val_loss: 44.9635\n",
      "Epoch 154/2000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 44.2161 - val_loss: 46.8922\n",
      "Epoch 155/2000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 42.5274 - val_loss: 44.1977\n",
      "Epoch 156/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 11ms/step - loss: 38.4373 - val_loss: 49.3048\n",
      "Epoch 157/2000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 42.1831 - val_loss: 45.7371\n",
      "Epoch 158/2000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 45.3581 - val_loss: 44.9804\n",
      "Epoch 159/2000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 40.7258 - val_loss: 48.3382\n",
      "Epoch 160/2000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 43.3024 - val_loss: 43.9138\n",
      "Epoch 161/2000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 40.7327 - val_loss: 46.1046\n",
      "Epoch 162/2000\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 41.2503 - val_loss: 44.4852\n",
      "Epoch 163/2000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 43.9664 - val_loss: 43.7740\n",
      "Epoch 164/2000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 40.3871 - val_loss: 44.0182\n",
      "Epoch 165/2000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 42.8343 - val_loss: 43.2968\n",
      "Epoch 166/2000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 42.2614 - val_loss: 44.9049\n",
      "Epoch 167/2000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 40.1080 - val_loss: 46.6022\n",
      "Epoch 168/2000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 41.7878 - val_loss: 43.3727\n",
      "Epoch 169/2000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 40.9829 - val_loss: 51.3334\n",
      "Epoch 170/2000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 43.1225 - val_loss: 43.1920\n",
      "Epoch 171/2000\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 39.1062 - val_loss: 45.9782\n",
      "Epoch 172/2000\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 44.7616 - val_loss: 47.7704\n",
      "Epoch 173/2000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 43.0468 - val_loss: 46.1010\n",
      "Epoch 174/2000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 39.8121 - val_loss: 46.1072\n",
      "Epoch 175/2000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 43.1404 - val_loss: 48.1637\n",
      "Epoch 176/2000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 40.8169 - val_loss: 43.5825\n",
      "Epoch 177/2000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 42.0505 - val_loss: 43.6224\n",
      "Epoch 178/2000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 39.5008 - val_loss: 45.5505\n",
      "Epoch 179/2000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 40.0500 - val_loss: 43.4731\n",
      "Epoch 180/2000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 40.2016 - val_loss: 45.5151\n",
      "Epoch 181/2000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 41.5812 - val_loss: 47.7158\n",
      "Epoch 182/2000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 46.7327 - val_loss: 43.6587\n",
      "Epoch 183/2000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 40.1932 - val_loss: 44.1778\n",
      "Epoch 184/2000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 43.8603 - val_loss: 46.5848\n",
      "Epoch 185/2000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 44.2423 - val_loss: 46.5712\n",
      "Epoch 186/2000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 39.8283 - val_loss: 43.6836\n",
      "Epoch 187/2000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 43.9011 - val_loss: 43.9933\n",
      "Epoch 188/2000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 42.9920 - val_loss: 49.3469\n",
      "Epoch 189/2000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 45.0501 - val_loss: 42.8533\n",
      "Epoch 190/2000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 39.7766 - val_loss: 46.8367\n",
      "Epoch 191/2000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 40.4259 - val_loss: 43.6148\n",
      "Epoch 192/2000\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 42.2243 - val_loss: 44.8394\n",
      "Epoch 193/2000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 39.5338 - val_loss: 43.0975\n",
      "Epoch 194/2000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 41.9031 - val_loss: 45.3865\n",
      "Epoch 195/2000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 46.2467 - val_loss: 43.0098\n",
      "Epoch 196/2000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 39.2682 - val_loss: 43.0898\n",
      "Epoch 197/2000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 41.5944 - val_loss: 45.5275\n",
      "Epoch 198/2000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 40.6137 - val_loss: 43.0871\n",
      "Epoch 199/2000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 40.3131 - val_loss: 46.6438\n",
      "Epoch 200/2000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 40.2844 - val_loss: 43.8974\n",
      "Epoch 201/2000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 41.6934 - val_loss: 42.9125\n",
      "Epoch 202/2000\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 41.6095 - val_loss: 47.9540\n",
      "Epoch 203/2000\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 39.3402 - val_loss: 43.1107\n",
      "Epoch 204/2000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 43.8425 - val_loss: 44.3002\n",
      "Epoch 205/2000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 39.4102 - val_loss: 43.8471\n",
      "Epoch 206/2000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 38.3432 - val_loss: 43.2978\n",
      "Epoch 207/2000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 44.0854 - val_loss: 43.9441\n",
      "Epoch 208/2000\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 41.3898 - val_loss: 45.2919\n",
      "Epoch 209/2000\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 40.5723 - val_loss: 46.8743\n",
      "Epoch 00209: early stopping\n",
      "Epoch 1/2000\n",
      "9/9 [==============================] - 1s 30ms/step - loss: 1589.3149 - val_loss: 395.5957\n",
      "Epoch 2/2000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 1580.2673 - val_loss: 639.1362\n",
      "Epoch 3/2000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 1572.4225 - val_loss: 797.2283\n",
      "Epoch 4/2000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 1565.8398 - val_loss: 912.1890\n",
      "Epoch 5/2000\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 1557.8188 - val_loss: 996.4860\n",
      "Epoch 6/2000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 1551.3763 - val_loss: 1063.0388\n",
      "Epoch 7/2000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 1543.8987 - val_loss: 1127.3641\n",
      "Epoch 8/2000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 1535.4719 - val_loss: 1181.2325\n",
      "Epoch 9/2000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 1526.9224 - val_loss: 1227.8674\n",
      "Epoch 10/2000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 1520.0964 - val_loss: 1271.5457\n",
      "Epoch 11/2000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 1511.7805 - val_loss: 1309.1758\n",
      "Epoch 12/2000\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 1501.5123 - val_loss: 1349.1697\n",
      "Epoch 13/2000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 1492.4592 - val_loss: 1384.3281\n",
      "Epoch 14/2000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 1482.8929 - val_loss: 1416.1888\n",
      "Epoch 15/2000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 1472.0504 - val_loss: 1445.5441\n",
      "Epoch 16/2000\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 1460.8851 - val_loss: 1466.7371\n",
      "Epoch 17/2000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 1451.2528 - val_loss: 1487.4661\n",
      "Epoch 18/2000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 1439.8304 - val_loss: 1505.6298\n",
      "Epoch 19/2000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 1427.5217 - val_loss: 1523.0679\n",
      "Epoch 20/2000\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 1415.3989 - val_loss: 1545.8859\n",
      "Epoch 21/2000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 1404.7141 - val_loss: 1552.8514\n",
      "Epoch 00021: early stopping\n",
      "3/3 [==============================] - 0s 9ms/step\n",
      "Epoch 1/2000\n",
      "9/9 [==============================] - 1s 31ms/step - loss: 1651.6075 - val_loss: 442.0420\n",
      "Epoch 2/2000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 1644.5537 - val_loss: 692.6390\n",
      "Epoch 3/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 12ms/step - loss: 1637.8062 - val_loss: 837.4780\n",
      "Epoch 4/2000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 1630.4908 - val_loss: 937.6667\n",
      "Epoch 5/2000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 1623.0229 - val_loss: 1013.8231\n",
      "Epoch 6/2000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 1615.0732 - val_loss: 1076.2643\n",
      "Epoch 7/2000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 1608.4518 - val_loss: 1120.3395\n",
      "Epoch 8/2000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 1599.8829 - val_loss: 1143.4652\n",
      "Epoch 9/2000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 1591.3433 - val_loss: 1165.4719\n",
      "Epoch 10/2000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 1582.5640 - val_loss: 1185.9998\n",
      "Epoch 11/2000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 1573.8463 - val_loss: 1212.7880\n",
      "Epoch 12/2000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 1564.5430 - val_loss: 1245.6460\n",
      "Epoch 13/2000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 1555.3387 - val_loss: 1276.6754\n",
      "Epoch 14/2000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 1545.0654 - val_loss: 1296.3644\n",
      "Epoch 15/2000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 1535.2831 - val_loss: 1306.4156\n",
      "Epoch 16/2000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 1524.7312 - val_loss: 1313.5914\n",
      "Epoch 17/2000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 1513.9072 - val_loss: 1322.1987\n",
      "Epoch 18/2000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 1502.6416 - val_loss: 1334.7307\n",
      "Epoch 19/2000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 1490.4170 - val_loss: 1341.2687\n",
      "Epoch 20/2000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 1478.9084 - val_loss: 1347.9592\n",
      "Epoch 21/2000\n",
      "9/9 [==============================] - 0s 16ms/step - loss: 1467.1544 - val_loss: 1352.6176\n",
      "Epoch 00021: early stopping\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "Epoch 1/2000\n",
      "9/9 [==============================] - 1s 30ms/step - loss: 1652.0696 - val_loss: 1401.3275\n",
      "Epoch 2/2000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 1643.3376 - val_loss: 1455.4661\n",
      "Epoch 3/2000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 1635.4044 - val_loss: 1488.3033\n",
      "Epoch 4/2000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 1626.3877 - val_loss: 1504.8539\n",
      "Epoch 5/2000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 1618.3361 - val_loss: 1521.5081\n",
      "Epoch 6/2000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 1609.8198 - val_loss: 1530.7205\n",
      "Epoch 7/2000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 1600.9822 - val_loss: 1535.2810\n",
      "Epoch 8/2000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 1592.5377 - val_loss: 1541.4015\n",
      "Epoch 9/2000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 1584.6022 - val_loss: 1547.9265\n",
      "Epoch 10/2000\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 1576.3685 - val_loss: 1548.4517\n",
      "Epoch 11/2000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 1568.2351 - val_loss: 1548.4844\n",
      "Epoch 12/2000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 1559.7065 - val_loss: 1543.8340\n",
      "Epoch 13/2000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 1551.1482 - val_loss: 1535.9824\n",
      "Epoch 14/2000\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 1541.9813 - val_loss: 1521.5149\n",
      "Epoch 15/2000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 1533.4540 - val_loss: 1505.7628\n",
      "Epoch 16/2000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 1524.7943 - val_loss: 1488.9738\n",
      "Epoch 17/2000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 1514.8008 - val_loss: 1479.2705\n",
      "Epoch 18/2000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 1505.1031 - val_loss: 1465.4060\n",
      "Epoch 19/2000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 1494.5435 - val_loss: 1454.5370\n",
      "Epoch 20/2000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 1482.9510 - val_loss: 1442.0731\n",
      "Epoch 21/2000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 1472.1157 - val_loss: 1424.9806\n",
      "Epoch 00021: early stopping\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "Epoch 1/2000\n",
      "9/9 [==============================] - 1s 32ms/step - loss: 1531.4839 - val_loss: 5541.8384\n",
      "Epoch 2/2000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 1526.0015 - val_loss: 3932.3008\n",
      "Epoch 3/2000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 1519.3386 - val_loss: 3304.5544\n",
      "Epoch 4/2000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 1514.1014 - val_loss: 2965.0911\n",
      "Epoch 5/2000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 1508.1288 - val_loss: 2731.7847\n",
      "Epoch 6/2000\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 1501.3070 - val_loss: 2571.2827\n",
      "Epoch 7/2000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 1494.6758 - val_loss: 2453.0308\n",
      "Epoch 8/2000\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 1487.2148 - val_loss: 2364.0154\n",
      "Epoch 9/2000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 1479.6908 - val_loss: 2285.0305\n",
      "Epoch 10/2000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 1472.3656 - val_loss: 2217.5249\n",
      "Epoch 11/2000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 1464.1625 - val_loss: 2153.4631\n",
      "Epoch 12/2000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 1455.9835 - val_loss: 2102.1313\n",
      "Epoch 13/2000\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 1447.5538 - val_loss: 2041.9695\n",
      "Epoch 14/2000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 1438.5371 - val_loss: 1993.2863\n",
      "Epoch 15/2000\n",
      "9/9 [==============================] - 0s 16ms/step - loss: 1427.9822 - val_loss: 1951.1570\n",
      "Epoch 16/2000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 1418.1501 - val_loss: 1906.5775\n",
      "Epoch 17/2000\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1408.0995 - val_loss: 1859.9678\n",
      "Epoch 18/2000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 1396.1941 - val_loss: 1817.2754\n",
      "Epoch 19/2000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 1385.2784 - val_loss: 1780.1167\n",
      "Epoch 20/2000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 1373.2374 - val_loss: 1739.0973\n",
      "Epoch 21/2000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 1362.0120 - val_loss: 1689.2401\n",
      "Epoch 22/2000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 1348.8854 - val_loss: 1646.8198\n",
      "Epoch 23/2000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 1336.2323 - val_loss: 1602.2224\n",
      "Epoch 24/2000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 1321.3706 - val_loss: 1557.8994\n",
      "Epoch 25/2000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 1307.4266 - val_loss: 1511.4194\n",
      "Epoch 26/2000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 1293.5051 - val_loss: 1471.7264\n",
      "Epoch 27/2000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 1279.0831 - val_loss: 1428.8298\n",
      "Epoch 28/2000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 1264.1136 - val_loss: 1375.3098\n",
      "Epoch 29/2000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 1249.8032 - val_loss: 1316.7357\n",
      "Epoch 30/2000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 1232.3107 - val_loss: 1262.9741\n",
      "Epoch 31/2000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 1217.0156 - val_loss: 1192.8064\n",
      "Epoch 32/2000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 1200.2770 - val_loss: 1133.2675\n",
      "Epoch 33/2000\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 1184.5312 - val_loss: 1073.7651\n",
      "Epoch 34/2000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 1168.3705 - val_loss: 1008.9724\n",
      "Epoch 35/2000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 1151.5348 - val_loss: 942.7733\n",
      "Epoch 36/2000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 1133.3126 - val_loss: 885.5428\n",
      "Epoch 37/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 11ms/step - loss: 1116.6787 - val_loss: 829.5233\n",
      "Epoch 38/2000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 1102.1531 - val_loss: 764.7782\n",
      "Epoch 39/2000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 1080.9819 - val_loss: 716.4703\n",
      "Epoch 40/2000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 1066.6370 - val_loss: 656.6973\n",
      "Epoch 41/2000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 1046.4663 - val_loss: 613.7878\n",
      "Epoch 42/2000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 1029.9221 - val_loss: 571.0283\n",
      "Epoch 43/2000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 1012.5709 - val_loss: 534.9085\n",
      "Epoch 44/2000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 997.8595 - val_loss: 482.6661\n",
      "Epoch 45/2000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 976.4050 - val_loss: 440.4130\n",
      "Epoch 46/2000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 958.5466 - val_loss: 412.2431\n",
      "Epoch 47/2000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 942.0101 - val_loss: 379.4873\n",
      "Epoch 48/2000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 925.4193 - val_loss: 354.6133\n",
      "Epoch 49/2000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 908.5814 - val_loss: 314.0677\n",
      "Epoch 50/2000\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 890.1907 - val_loss: 292.3910\n",
      "Epoch 51/2000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 873.0969 - val_loss: 268.5197\n",
      "Epoch 52/2000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 855.5111 - val_loss: 253.4098\n",
      "Epoch 53/2000\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 839.9966 - val_loss: 244.3218\n",
      "Epoch 54/2000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 822.5374 - val_loss: 230.4340\n",
      "Epoch 55/2000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 805.2303 - val_loss: 219.2646\n",
      "Epoch 56/2000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 788.3654 - val_loss: 216.2930\n",
      "Epoch 57/2000\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 776.4863 - val_loss: 201.0948\n",
      "Epoch 58/2000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 758.2532 - val_loss: 206.1189\n",
      "Epoch 59/2000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 741.9410 - val_loss: 205.5259\n",
      "Epoch 60/2000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 726.5960 - val_loss: 193.5950\n",
      "Epoch 61/2000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 711.9937 - val_loss: 184.5135\n",
      "Epoch 62/2000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 694.8223 - val_loss: 177.7937\n",
      "Epoch 63/2000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 680.4016 - val_loss: 185.4525\n",
      "Epoch 64/2000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 667.4343 - val_loss: 188.1255\n",
      "Epoch 65/2000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 649.5011 - val_loss: 176.3346\n",
      "Epoch 66/2000\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 637.8427 - val_loss: 168.2306\n",
      "Epoch 67/2000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 627.0223 - val_loss: 171.7180\n",
      "Epoch 68/2000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 612.0367 - val_loss: 176.5479\n",
      "Epoch 69/2000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 593.3160 - val_loss: 175.3878\n",
      "Epoch 70/2000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 580.3871 - val_loss: 184.9102\n",
      "Epoch 71/2000\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 562.9810 - val_loss: 191.6660\n",
      "Epoch 72/2000\n",
      "9/9 [==============================] - 0s 16ms/step - loss: 549.7496 - val_loss: 199.5794\n",
      "Epoch 73/2000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 537.2472 - val_loss: 207.7512\n",
      "Epoch 74/2000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 527.0318 - val_loss: 201.2554\n",
      "Epoch 75/2000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 513.2336 - val_loss: 202.3310\n",
      "Epoch 76/2000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 501.1736 - val_loss: 203.6090\n",
      "Epoch 77/2000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 483.6575 - val_loss: 201.0289\n",
      "Epoch 78/2000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 470.7159 - val_loss: 210.1228\n",
      "Epoch 79/2000\n",
      "9/9 [==============================] - 0s 16ms/step - loss: 465.3563 - val_loss: 188.4566\n",
      "Epoch 80/2000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 453.2022 - val_loss: 199.2681\n",
      "Epoch 81/2000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 435.6693 - val_loss: 214.9851\n",
      "Epoch 82/2000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 429.1240 - val_loss: 207.4149\n",
      "Epoch 83/2000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 414.2558 - val_loss: 189.7601\n",
      "Epoch 84/2000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 401.9319 - val_loss: 202.6282\n",
      "Epoch 85/2000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 394.2730 - val_loss: 203.8497\n",
      "Epoch 86/2000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 377.3468 - val_loss: 190.9408\n",
      "Epoch 00086: early stopping\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "Epoch 1/2000\n",
      "9/9 [==============================] - 1s 31ms/step - loss: 1602.2546 - val_loss: 2864.0686\n",
      "Epoch 2/2000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 1596.6041 - val_loss: 2393.9966\n",
      "Epoch 3/2000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 1589.8960 - val_loss: 2192.8450\n",
      "Epoch 4/2000\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 1584.3298 - val_loss: 2069.7690\n",
      "Epoch 5/2000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 1578.6740 - val_loss: 1992.0099\n",
      "Epoch 6/2000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 1570.0521 - val_loss: 1924.6046\n",
      "Epoch 7/2000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 1564.5032 - val_loss: 1867.2540\n",
      "Epoch 8/2000\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 1557.1445 - val_loss: 1820.9266\n",
      "Epoch 9/2000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 1548.0867 - val_loss: 1776.5615\n",
      "Epoch 10/2000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 1540.5231 - val_loss: 1737.9208\n",
      "Epoch 11/2000\n",
      "9/9 [==============================] - 0s 16ms/step - loss: 1532.8287 - val_loss: 1698.0686\n",
      "Epoch 12/2000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 1522.9351 - val_loss: 1663.3423\n",
      "Epoch 13/2000\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 1513.3054 - val_loss: 1622.2170\n",
      "Epoch 14/2000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 1503.7360 - val_loss: 1586.3542\n",
      "Epoch 15/2000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 1492.6102 - val_loss: 1545.2910\n",
      "Epoch 16/2000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 1481.4396 - val_loss: 1504.0902\n",
      "Epoch 17/2000\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 1470.5343 - val_loss: 1466.5139\n",
      "Epoch 18/2000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 1458.4331 - val_loss: 1427.5951\n",
      "Epoch 19/2000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 1446.9163 - val_loss: 1390.5415\n",
      "Epoch 20/2000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 1434.7351 - val_loss: 1362.8304\n",
      "Epoch 21/2000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 1422.8127 - val_loss: 1344.3751\n",
      "Epoch 22/2000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 1407.4271 - val_loss: 1320.3451\n",
      "Epoch 23/2000\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 1393.8882 - val_loss: 1295.8186\n",
      "Epoch 24/2000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 1380.0641 - val_loss: 1267.7028\n",
      "Epoch 25/2000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 1364.2484 - val_loss: 1235.8644\n",
      "Epoch 26/2000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 1349.7172 - val_loss: 1204.6385\n",
      "Epoch 27/2000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 1334.5559 - val_loss: 1163.8993\n",
      "Epoch 28/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 11ms/step - loss: 1318.2970 - val_loss: 1131.3398\n",
      "Epoch 29/2000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 1303.5270 - val_loss: 1101.1278\n",
      "Epoch 30/2000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 1284.7825 - val_loss: 1076.9470\n",
      "Epoch 31/2000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 1271.3267 - val_loss: 1045.9153\n",
      "Epoch 32/2000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 1253.7150 - val_loss: 1041.3641\n",
      "Epoch 33/2000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 1237.1011 - val_loss: 1028.3119\n",
      "Epoch 34/2000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 1219.6438 - val_loss: 1001.9919\n",
      "Epoch 35/2000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 1203.4083 - val_loss: 983.5321\n",
      "Epoch 36/2000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 1186.0795 - val_loss: 958.5378\n",
      "Epoch 37/2000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 1168.3899 - val_loss: 927.5140\n",
      "Epoch 38/2000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 1149.9259 - val_loss: 903.0069\n",
      "Epoch 39/2000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 1130.3601 - val_loss: 878.1155\n",
      "Epoch 40/2000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 1113.3381 - val_loss: 842.0822\n",
      "Epoch 41/2000\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 1095.3856 - val_loss: 808.7241\n",
      "Epoch 42/2000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 1078.9122 - val_loss: 776.5191\n",
      "Epoch 43/2000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 1060.5887 - val_loss: 728.5068\n",
      "Epoch 44/2000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 1039.9237 - val_loss: 704.1089\n",
      "Epoch 45/2000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 1022.3583 - val_loss: 657.9670\n",
      "Epoch 46/2000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 1004.3315 - val_loss: 610.8649\n",
      "Epoch 47/2000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 986.1292 - val_loss: 595.0340\n",
      "Epoch 48/2000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 968.2892 - val_loss: 568.1815\n",
      "Epoch 49/2000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 950.4330 - val_loss: 539.0501\n",
      "Epoch 50/2000\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 932.7382 - val_loss: 534.1700\n",
      "Epoch 51/2000\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 916.7190 - val_loss: 508.1043\n",
      "Epoch 52/2000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 901.0199 - val_loss: 507.4647\n",
      "Epoch 53/2000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 883.6466 - val_loss: 476.9709\n",
      "Epoch 54/2000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 866.3444 - val_loss: 459.0741\n",
      "Epoch 55/2000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 848.8862 - val_loss: 433.0223\n",
      "Epoch 56/2000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 832.1400 - val_loss: 400.5854\n",
      "Epoch 57/2000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 809.3629 - val_loss: 397.5632\n",
      "Epoch 58/2000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 798.5696 - val_loss: 375.3105\n",
      "Epoch 59/2000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 777.8551 - val_loss: 369.2263\n",
      "Epoch 60/2000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 760.2960 - val_loss: 364.2811\n",
      "Epoch 61/2000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 746.5719 - val_loss: 350.4105\n",
      "Epoch 62/2000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 731.0519 - val_loss: 340.6698\n",
      "Epoch 63/2000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 714.0379 - val_loss: 337.4522\n",
      "Epoch 64/2000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 702.0716 - val_loss: 338.0610\n",
      "Epoch 65/2000\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 684.1258 - val_loss: 309.8385\n",
      "Epoch 66/2000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 664.3105 - val_loss: 308.6704\n",
      "Epoch 67/2000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 649.9196 - val_loss: 279.8651\n",
      "Epoch 68/2000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 637.5415 - val_loss: 309.0495\n",
      "Epoch 69/2000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 619.4453 - val_loss: 310.5124\n",
      "Epoch 70/2000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 604.0318 - val_loss: 305.7395\n",
      "Epoch 71/2000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 588.8117 - val_loss: 299.8473\n",
      "Epoch 72/2000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 573.9167 - val_loss: 303.7783\n",
      "Epoch 73/2000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 563.0114 - val_loss: 274.0570\n",
      "Epoch 74/2000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 549.5003 - val_loss: 253.6772\n",
      "Epoch 75/2000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 531.7183 - val_loss: 234.1902\n",
      "Epoch 76/2000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 518.8814 - val_loss: 245.3897\n",
      "Epoch 77/2000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 506.5486 - val_loss: 220.1577\n",
      "Epoch 78/2000\n",
      "9/9 [==============================] - 0s 16ms/step - loss: 492.6326 - val_loss: 230.6087\n",
      "Epoch 79/2000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 478.8218 - val_loss: 250.0156\n",
      "Epoch 80/2000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 465.3520 - val_loss: 245.8059\n",
      "Epoch 81/2000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 449.9096 - val_loss: 216.8717\n",
      "Epoch 82/2000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 439.5586 - val_loss: 200.0121\n",
      "Epoch 83/2000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 425.7726 - val_loss: 189.1379\n",
      "Epoch 84/2000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 416.9987 - val_loss: 221.2569\n",
      "Epoch 85/2000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 398.5063 - val_loss: 235.1759\n",
      "Epoch 86/2000\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 390.4076 - val_loss: 213.1850\n",
      "Epoch 87/2000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 377.7682 - val_loss: 200.3962\n",
      "Epoch 88/2000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 370.2101 - val_loss: 169.7508\n",
      "Epoch 89/2000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 355.7387 - val_loss: 160.6108\n",
      "Epoch 90/2000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 341.7898 - val_loss: 148.9682\n",
      "Epoch 91/2000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 336.5351 - val_loss: 144.0453\n",
      "Epoch 92/2000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 324.7465 - val_loss: 152.4861\n",
      "Epoch 93/2000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 309.9642 - val_loss: 143.4176\n",
      "Epoch 94/2000\n",
      "9/9 [==============================] - 0s 16ms/step - loss: 304.8611 - val_loss: 148.5629\n",
      "Epoch 95/2000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 291.8720 - val_loss: 132.2895\n",
      "Epoch 96/2000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 280.9008 - val_loss: 135.3624\n",
      "Epoch 97/2000\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 273.0216 - val_loss: 143.4478\n",
      "Epoch 98/2000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 262.1473 - val_loss: 143.9674\n",
      "Epoch 99/2000\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 253.3277 - val_loss: 150.7759\n",
      "Epoch 100/2000\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 246.2074 - val_loss: 127.8476\n",
      "Epoch 101/2000\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 238.1523 - val_loss: 112.1314\n",
      "Epoch 102/2000\n",
      "9/9 [==============================] - 0s 16ms/step - loss: 230.5470 - val_loss: 115.4923\n",
      "Epoch 103/2000\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 220.1785 - val_loss: 115.9414\n",
      "Epoch 104/2000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 214.0449 - val_loss: 104.8048\n",
      "Epoch 105/2000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 203.8958 - val_loss: 107.8946\n",
      "Epoch 106/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 11ms/step - loss: 200.0712 - val_loss: 105.1178\n",
      "Epoch 107/2000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 190.0958 - val_loss: 96.1214\n",
      "Epoch 108/2000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 186.5228 - val_loss: 97.0166\n",
      "Epoch 109/2000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 179.5111 - val_loss: 88.9700\n",
      "Epoch 110/2000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 174.7970 - val_loss: 97.4096\n",
      "Epoch 111/2000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 163.1622 - val_loss: 83.4545\n",
      "Epoch 112/2000\n",
      "9/9 [==============================] - 0s 16ms/step - loss: 160.7828 - val_loss: 76.9211\n",
      "Epoch 113/2000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 155.3980 - val_loss: 83.2763\n",
      "Epoch 114/2000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 150.4912 - val_loss: 75.6032\n",
      "Epoch 115/2000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 143.2936 - val_loss: 76.2717\n",
      "Epoch 116/2000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 139.7690 - val_loss: 79.2969\n",
      "Epoch 117/2000\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 133.3086 - val_loss: 67.7303\n",
      "Epoch 118/2000\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 126.6779 - val_loss: 65.0555\n",
      "Epoch 119/2000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 122.0931 - val_loss: 66.0079\n",
      "Epoch 120/2000\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 118.8547 - val_loss: 62.5441\n",
      "Epoch 121/2000\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 115.3341 - val_loss: 61.7725\n",
      "Epoch 122/2000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 111.7958 - val_loss: 63.9981\n",
      "Epoch 123/2000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 107.5075 - val_loss: 62.2549\n",
      "Epoch 124/2000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 107.8605 - val_loss: 59.5088\n",
      "Epoch 125/2000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 101.4793 - val_loss: 56.3035\n",
      "Epoch 126/2000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 99.1632 - val_loss: 57.3022\n",
      "Epoch 127/2000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 95.1777 - val_loss: 57.8077\n",
      "Epoch 128/2000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 92.1888 - val_loss: 57.5888\n",
      "Epoch 129/2000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 88.8879 - val_loss: 54.9881\n",
      "Epoch 130/2000\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 85.2096 - val_loss: 53.2878\n",
      "Epoch 131/2000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 83.4204 - val_loss: 52.9998\n",
      "Epoch 132/2000\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 81.8411 - val_loss: 51.7887\n",
      "Epoch 133/2000\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 79.6314 - val_loss: 54.4696\n",
      "Epoch 134/2000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 75.6924 - val_loss: 51.7309\n",
      "Epoch 135/2000\n",
      "9/9 [==============================] - 0s 16ms/step - loss: 75.6825 - val_loss: 52.0479\n",
      "Epoch 136/2000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 73.7932 - val_loss: 58.0175\n",
      "Epoch 137/2000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 73.6941 - val_loss: 50.1530\n",
      "Epoch 138/2000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 72.4053 - val_loss: 53.4220\n",
      "Epoch 139/2000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 68.7970 - val_loss: 54.8964\n",
      "Epoch 140/2000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 65.6169 - val_loss: 48.9640\n",
      "Epoch 141/2000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 65.4006 - val_loss: 53.0902\n",
      "Epoch 142/2000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 65.0533 - val_loss: 56.8830\n",
      "Epoch 143/2000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 64.7298 - val_loss: 50.0789\n",
      "Epoch 144/2000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 61.8590 - val_loss: 54.1615\n",
      "Epoch 145/2000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 60.9065 - val_loss: 51.6682\n",
      "Epoch 146/2000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 63.1344 - val_loss: 52.4739\n",
      "Epoch 147/2000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 56.4829 - val_loss: 51.2244\n",
      "Epoch 148/2000\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 58.7623 - val_loss: 49.9061\n",
      "Epoch 149/2000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 59.1850 - val_loss: 52.2185\n",
      "Epoch 150/2000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 54.8694 - val_loss: 53.9812\n",
      "Epoch 151/2000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 52.6219 - val_loss: 51.1922\n",
      "Epoch 152/2000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 52.4854 - val_loss: 51.4211\n",
      "Epoch 153/2000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 52.5421 - val_loss: 52.2570\n",
      "Epoch 154/2000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 54.8495 - val_loss: 52.7837\n",
      "Epoch 155/2000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 48.3852 - val_loss: 50.1012\n",
      "Epoch 156/2000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 49.1433 - val_loss: 52.9318\n",
      "Epoch 157/2000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 48.2762 - val_loss: 49.5062\n",
      "Epoch 158/2000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 50.4991 - val_loss: 52.1480\n",
      "Epoch 159/2000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 51.6135 - val_loss: 51.4504\n",
      "Epoch 160/2000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 48.5700 - val_loss: 55.1117\n",
      "Epoch 00160: early stopping\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb1471e2310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "3/3 [==============================] - 0s 4ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StackingRegressor(estimators=[('ELNE', ElasticNet(alpha=0.01)),\n",
       "                              ('KERR',\n",
       "                               <keras.wrappers.scikit_learn.KerasRegressor object at 0x7fb1645e2f40>)],\n",
       "                  final_estimator=LinearRegression())"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "given-jamaica",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb147196a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 110ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([24.19403002, 45.78092171])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_test[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "republican-thriller",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 35ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input arrays must have same number of dimensions, but the array at index 0 has 2 dimension(s) and the array at index 1 has 1 dimension(s)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/utils/metaestimators.py:113\u001b[0m, in \u001b[0;36m_AvailableIfDescriptor.__get__.<locals>.<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m attr_err\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;66;03m# lambda, but not partial, allows help() to work with update_wrapper\u001b[39;00m\n\u001b[0;32m--> 113\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfn\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/ensemble/_stacking.py:267\u001b[0m, in \u001b[0;36m_BaseStacking.predict\u001b[0;34m(self, X, **predict_params)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;124;03m\"\"\"Predict target for X.\u001b[39;00m\n\u001b[1;32m    247\u001b[0m \n\u001b[1;32m    248\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;124;03m    Predicted targets.\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    266\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 267\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_estimator_\u001b[38;5;241m.\u001b[39mpredict(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpredict_params)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/ensemble/_stacking.py:775\u001b[0m, in \u001b[0;36mStackingRegressor.transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m    762\u001b[0m     \u001b[38;5;124;03m\"\"\"Return the predictions for X for each estimator.\u001b[39;00m\n\u001b[1;32m    763\u001b[0m \n\u001b[1;32m    764\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    773\u001b[0m \u001b[38;5;124;03m        Prediction outputs for each estimator.\u001b[39;00m\n\u001b[1;32m    774\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/ensemble/_stacking.py:242\u001b[0m, in \u001b[0;36m_BaseStacking._transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    236\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    237\u001b[0m predictions \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28mgetattr\u001b[39m(est, meth)(X)\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m est, meth \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstack_method_)\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m est \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrop\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    241\u001b[0m ]\n\u001b[0;32m--> 242\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_concatenate_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/ensemble/_stacking.py:101\u001b[0m, in \u001b[0;36m_BaseStacking._concatenate_predictions\u001b[0;34m(self, X, predictions)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39missparse(X):\n\u001b[1;32m     99\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39mhstack(X_meta, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39mX\u001b[38;5;241m.\u001b[39mformat)\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_meta\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36mhstack\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/numpy/core/shape_base.py:345\u001b[0m, in \u001b[0;36mhstack\u001b[0;34m(tup)\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _nx\u001b[38;5;241m.\u001b[39mconcatenate(arrs, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    344\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_nx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: all the input arrays must have same number of dimensions, but the array at index 0 has 2 dimension(s) and the array at index 1 has 1 dimension(s)"
     ]
    }
   ],
   "source": [
    "model.predict(X_test[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "pleasant-competition",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_K = K_R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "auburn-translation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "11/11 [==============================] - 1s 26ms/step - loss: 1595.5521 - val_loss: 1739.2104\n",
      "Epoch 2/2000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 1587.6556 - val_loss: 1714.5410\n",
      "Epoch 3/2000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1579.7306 - val_loss: 1709.0000\n",
      "Epoch 4/2000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 1571.7888 - val_loss: 1712.1527\n",
      "Epoch 5/2000\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 1562.8529 - val_loss: 1701.8569\n",
      "Epoch 6/2000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1553.4917 - val_loss: 1691.4114\n",
      "Epoch 7/2000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 1543.2665 - val_loss: 1686.1919\n",
      "Epoch 8/2000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1532.7073 - val_loss: 1665.6783\n",
      "Epoch 9/2000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 1520.7286 - val_loss: 1620.1686\n",
      "Epoch 10/2000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 1507.7292 - val_loss: 1579.4073\n",
      "Epoch 11/2000\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 1495.7214 - val_loss: 1530.2855\n",
      "Epoch 12/2000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1481.7000 - val_loss: 1481.7979\n",
      "Epoch 13/2000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1467.2277 - val_loss: 1436.4495\n",
      "Epoch 14/2000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 1453.0544 - val_loss: 1376.8263\n",
      "Epoch 15/2000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 1438.3722 - val_loss: 1319.4027\n",
      "Epoch 16/2000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 1420.4932 - val_loss: 1278.4294\n",
      "Epoch 17/2000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 1403.3293 - val_loss: 1232.3766\n",
      "Epoch 18/2000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 1388.6890 - val_loss: 1191.8075\n",
      "Epoch 19/2000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 1368.6479 - val_loss: 1155.5981\n",
      "Epoch 20/2000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 1351.8937 - val_loss: 1127.0712\n",
      "Epoch 21/2000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 1332.6495 - val_loss: 1078.2097\n",
      "Epoch 22/2000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 1312.5177 - val_loss: 1014.5941\n",
      "Epoch 23/2000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 1293.1230 - val_loss: 975.4785\n",
      "Epoch 24/2000\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 1274.1567 - val_loss: 944.3383\n",
      "Epoch 25/2000\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 1253.9924 - val_loss: 878.9377\n",
      "Epoch 26/2000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 1235.4297 - val_loss: 846.6194\n",
      "Epoch 27/2000\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 1212.4814 - val_loss: 824.0738\n",
      "Epoch 28/2000\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 1192.3674 - val_loss: 786.3708\n",
      "Epoch 29/2000\n",
      "11/11 [==============================] - 0s 22ms/step - loss: 1173.7804 - val_loss: 738.1202\n",
      "Epoch 30/2000\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1151.2424 - val_loss: 680.2272\n",
      "Epoch 31/2000\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 1129.0518 - val_loss: 652.9841\n",
      "Epoch 32/2000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 1106.4681 - val_loss: 617.7556\n",
      "Epoch 33/2000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1087.2601 - val_loss: 586.0150\n",
      "Epoch 34/2000\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 1064.8391 - val_loss: 564.0128\n",
      "Epoch 35/2000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1042.8783 - val_loss: 520.3185\n",
      "Epoch 36/2000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 1020.9277 - val_loss: 476.7598\n",
      "Epoch 37/2000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 1000.2808 - val_loss: 457.1234\n",
      "Epoch 38/2000\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 979.4369 - val_loss: 429.9579\n",
      "Epoch 39/2000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 956.1827 - val_loss: 415.0224\n",
      "Epoch 40/2000\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 934.4667 - val_loss: 393.8934\n",
      "Epoch 41/2000\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 915.1312 - val_loss: 361.4267\n",
      "Epoch 42/2000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 891.1291 - val_loss: 358.5780\n",
      "Epoch 43/2000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 872.0526 - val_loss: 339.2401\n",
      "Epoch 44/2000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 849.8078 - val_loss: 300.2025\n",
      "Epoch 45/2000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 827.9717 - val_loss: 304.9666\n",
      "Epoch 46/2000\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 807.7586 - val_loss: 336.5308\n",
      "Epoch 47/2000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 786.7874 - val_loss: 333.8384\n",
      "Epoch 48/2000\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 770.7754 - val_loss: 336.7728\n",
      "Epoch 49/2000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 747.3167 - val_loss: 323.5132\n",
      "Epoch 50/2000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 727.9229 - val_loss: 313.0071\n",
      "Epoch 51/2000\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 710.2659 - val_loss: 334.8034\n",
      "Epoch 52/2000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 689.6179 - val_loss: 348.2527\n",
      "Epoch 53/2000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 667.2216 - val_loss: 356.9900\n",
      "Epoch 54/2000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 648.9558 - val_loss: 334.3060\n",
      "Epoch 55/2000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 629.8175 - val_loss: 341.9596\n",
      "Epoch 56/2000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 611.5245 - val_loss: 315.5483\n",
      "Epoch 57/2000\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 591.4173 - val_loss: 315.5424\n",
      "Epoch 58/2000\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 575.8890 - val_loss: 296.1424\n",
      "Epoch 59/2000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 558.8243 - val_loss: 285.9221\n",
      "Epoch 60/2000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 542.3658 - val_loss: 281.1349\n",
      "Epoch 61/2000\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 521.1403 - val_loss: 248.7919\n",
      "Epoch 62/2000\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 506.5850 - val_loss: 245.8765\n",
      "Epoch 63/2000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 490.5137 - val_loss: 211.4987\n",
      "Epoch 64/2000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 470.6243 - val_loss: 233.9787\n",
      "Epoch 65/2000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 455.6827 - val_loss: 236.7771\n",
      "Epoch 66/2000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 440.7363 - val_loss: 246.2717\n",
      "Epoch 67/2000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 426.6293 - val_loss: 229.6588\n",
      "Epoch 68/2000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 410.4615 - val_loss: 216.9701\n",
      "Epoch 69/2000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 392.8835 - val_loss: 191.3969\n",
      "Epoch 70/2000\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 381.3458 - val_loss: 198.4532\n",
      "Epoch 71/2000\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 361.8035 - val_loss: 191.8661\n",
      "Epoch 72/2000\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 351.2402 - val_loss: 186.2718\n",
      "Epoch 73/2000\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 336.5725 - val_loss: 180.0427\n",
      "Epoch 74/2000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 322.6782 - val_loss: 171.1727\n",
      "Epoch 75/2000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 310.7650 - val_loss: 190.0993\n",
      "Epoch 76/2000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 298.2851 - val_loss: 171.3579\n",
      "Epoch 77/2000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 288.3687 - val_loss: 163.6039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/2000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 271.9914 - val_loss: 169.9856\n",
      "Epoch 79/2000\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 262.5677 - val_loss: 155.8147\n",
      "Epoch 80/2000\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 251.9163 - val_loss: 171.4501\n",
      "Epoch 81/2000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 245.2926 - val_loss: 130.3772\n",
      "Epoch 82/2000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 233.5275 - val_loss: 147.8282\n",
      "Epoch 83/2000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 223.2791 - val_loss: 141.5839\n",
      "Epoch 84/2000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 214.4541 - val_loss: 138.8086\n",
      "Epoch 85/2000\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 201.9308 - val_loss: 138.7660\n",
      "Epoch 86/2000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 194.0577 - val_loss: 149.7583\n",
      "Epoch 87/2000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 187.2604 - val_loss: 121.5365\n",
      "Epoch 88/2000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 177.2330 - val_loss: 123.4568\n",
      "Epoch 89/2000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 168.6015 - val_loss: 124.0923\n",
      "Epoch 90/2000\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 163.3282 - val_loss: 130.2543\n",
      "Epoch 91/2000\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 152.3810 - val_loss: 115.0556\n",
      "Epoch 92/2000\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 146.2742 - val_loss: 108.5694\n",
      "Epoch 93/2000\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 142.7766 - val_loss: 110.1415\n",
      "Epoch 94/2000\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 134.6774 - val_loss: 112.8300\n",
      "Epoch 95/2000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 128.5834 - val_loss: 96.0472\n",
      "Epoch 96/2000\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 122.0663 - val_loss: 94.3353\n",
      "Epoch 97/2000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 121.6437 - val_loss: 82.6728\n",
      "Epoch 98/2000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 116.9679 - val_loss: 85.4102\n",
      "Epoch 99/2000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 106.9625 - val_loss: 81.3530\n",
      "Epoch 100/2000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 102.9840 - val_loss: 80.3076\n",
      "Epoch 101/2000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 99.3673 - val_loss: 78.2528\n",
      "Epoch 102/2000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 94.8497 - val_loss: 80.3977\n",
      "Epoch 103/2000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 92.0414 - val_loss: 72.0253\n",
      "Epoch 104/2000\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 88.0828 - val_loss: 63.2717\n",
      "Epoch 105/2000\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 85.1215 - val_loss: 58.7997\n",
      "Epoch 106/2000\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 80.2525 - val_loss: 67.1054\n",
      "Epoch 107/2000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 78.5415 - val_loss: 62.0336\n",
      "Epoch 108/2000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 75.6438 - val_loss: 60.2559\n",
      "Epoch 109/2000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 75.3901 - val_loss: 58.5149\n",
      "Epoch 110/2000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 71.9575 - val_loss: 57.7994\n",
      "Epoch 111/2000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 71.4169 - val_loss: 57.6840\n",
      "Epoch 112/2000\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 66.4706 - val_loss: 53.8351\n",
      "Epoch 113/2000\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 67.8557 - val_loss: 53.5765\n",
      "Epoch 114/2000\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 63.2937 - val_loss: 51.2021\n",
      "Epoch 115/2000\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 62.7877 - val_loss: 50.3674\n",
      "Epoch 116/2000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 61.6493 - val_loss: 49.6489\n",
      "Epoch 117/2000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 56.8138 - val_loss: 50.1960\n",
      "Epoch 118/2000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 60.3864 - val_loss: 48.3774\n",
      "Epoch 119/2000\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 54.2583 - val_loss: 47.5401\n",
      "Epoch 120/2000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 53.9890 - val_loss: 51.0605\n",
      "Epoch 121/2000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 54.5563 - val_loss: 48.6909\n",
      "Epoch 122/2000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 57.2128 - val_loss: 52.2551\n",
      "Epoch 123/2000\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 51.4027 - val_loss: 47.8570\n",
      "Epoch 124/2000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 49.2761 - val_loss: 48.5631\n",
      "Epoch 125/2000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 48.9757 - val_loss: 47.2908\n",
      "Epoch 126/2000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 48.6293 - val_loss: 47.4736\n",
      "Epoch 127/2000\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 48.1395 - val_loss: 46.9320\n",
      "Epoch 128/2000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 49.2907 - val_loss: 51.4825\n",
      "Epoch 129/2000\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 47.5727 - val_loss: 45.5035\n",
      "Epoch 130/2000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 50.6044 - val_loss: 45.7393\n",
      "Epoch 131/2000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 48.8108 - val_loss: 46.9656\n",
      "Epoch 132/2000\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 46.1250 - val_loss: 48.3042\n",
      "Epoch 133/2000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 44.9622 - val_loss: 45.6124\n",
      "Epoch 134/2000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 50.7882 - val_loss: 46.2219\n",
      "Epoch 135/2000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 45.3435 - val_loss: 45.4801\n",
      "Epoch 136/2000\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 45.5404 - val_loss: 45.1705\n",
      "Epoch 137/2000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 45.7298 - val_loss: 47.6616\n",
      "Epoch 138/2000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 48.1360 - val_loss: 45.9003\n",
      "Epoch 139/2000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 46.0110 - val_loss: 45.8770\n",
      "Epoch 140/2000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 48.2497 - val_loss: 49.6918\n",
      "Epoch 141/2000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 43.7850 - val_loss: 45.4552\n",
      "Epoch 142/2000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 45.5924 - val_loss: 45.5063\n",
      "Epoch 143/2000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 46.4024 - val_loss: 44.7839\n",
      "Epoch 144/2000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 45.2922 - val_loss: 44.8259\n",
      "Epoch 145/2000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 46.9471 - val_loss: 44.8557\n",
      "Epoch 146/2000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 45.5155 - val_loss: 48.0910\n",
      "Epoch 147/2000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 47.7755 - val_loss: 44.7848\n",
      "Epoch 148/2000\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 44.3125 - val_loss: 47.2091\n",
      "Epoch 149/2000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 45.6569 - val_loss: 44.8426\n",
      "Epoch 150/2000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 45.8791 - val_loss: 44.6770\n",
      "Epoch 151/2000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 42.5339 - val_loss: 45.4389\n",
      "Epoch 152/2000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 42.8797 - val_loss: 46.3606\n",
      "Epoch 153/2000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 43.4808 - val_loss: 44.1776\n",
      "Epoch 154/2000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 48.6479 - val_loss: 48.2482\n",
      "Epoch 155/2000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 45.8589 - val_loss: 46.1585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 156/2000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 42.1537 - val_loss: 50.4841\n",
      "Epoch 157/2000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 43.9226 - val_loss: 43.6672\n",
      "Epoch 158/2000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 43.4223 - val_loss: 44.9582\n",
      "Epoch 159/2000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 42.8841 - val_loss: 45.0228\n",
      "Epoch 160/2000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 45.8298 - val_loss: 44.4025\n",
      "Epoch 161/2000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 43.7008 - val_loss: 47.6002\n",
      "Epoch 162/2000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 42.3005 - val_loss: 45.0934\n",
      "Epoch 163/2000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 45.4406 - val_loss: 44.3587\n",
      "Epoch 164/2000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 43.0125 - val_loss: 45.5621\n",
      "Epoch 165/2000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 48.0194 - val_loss: 44.0060\n",
      "Epoch 166/2000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 46.1061 - val_loss: 43.7368\n",
      "Epoch 167/2000\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 42.1660 - val_loss: 46.7841\n",
      "Epoch 168/2000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 42.2855 - val_loss: 45.0488\n",
      "Epoch 169/2000\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 42.0859 - val_loss: 49.0780\n",
      "Epoch 170/2000\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 47.9934 - val_loss: 45.5054\n",
      "Epoch 171/2000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 43.4368 - val_loss: 43.7921\n",
      "Epoch 172/2000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 41.0334 - val_loss: 45.1761\n",
      "Epoch 173/2000\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 45.2484 - val_loss: 44.8287\n",
      "Epoch 174/2000\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 43.5164 - val_loss: 44.3487\n",
      "Epoch 175/2000\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 40.7682 - val_loss: 45.8642\n",
      "Epoch 176/2000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 40.1081 - val_loss: 44.6970\n",
      "Epoch 177/2000\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 46.0250 - val_loss: 45.9750\n",
      "Epoch 00177: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb1441e3a30>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_K.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "closed-bracket",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 100ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(22.85553, dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_K.predict(X_test[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "noted-control",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 33ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([22.855536, 41.029778], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_K.predict(X_test[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "usual-meditation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EZStacking",
   "language": "python",
   "name": "ezstacking"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
